{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn \n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\"users\":[]}\n",
    "with open('DATA.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(new_data, filename='DATA.json'):\n",
    "    with open(filename,'r+') as file:\n",
    "          # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data[\"users\"].append(new_data)\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        json.dump(file_data, file, indent = 4)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr=pd.read_csv('Medical_dataset/Training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt=pd.read_csv('Medical_dataset/Testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symp=[]\n",
    "disease=[]\n",
    "for i in range(len(df_tr)):\n",
    "    symp.append(df_tr.columns[df_tr.iloc[i]==1].to_list())\n",
    "    disease.append(df_tr.iloc[i,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I- GET ALL SYMPTOMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_symp_col=list(df_tr.columns[:-1])\n",
    "def clean_symp(sym):\n",
    "    return sym.replace('_',' ').replace('.1','').replace('(typhos)','').replace('yellowish','yellow').replace('yellowing','yellow') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_symp=[clean_symp(sym) for sym in (all_symp_col)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all symptoms which do not have a synset\n",
    "ohne_syns=[]\n",
    "mit_syns=[]\n",
    "for sym in all_symp:\n",
    "    if not wn.synsets(sym) :\n",
    "        ohne_syns.append(sym)\n",
    "    else:\n",
    "        mit_syns.append(sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mit_syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ohne_syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II- Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    nlp_doc=nlp(doc)\n",
    "    d=[]\n",
    "    for token in nlp_doc:\n",
    "        if(not token.text.lower()  in STOP_WORDS and  token.text.isalpha()):\n",
    "            d.append(token.lemma_.lower() )\n",
    "    return ' '.join(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sym(doc):\n",
    "    nlp_doc=nlp(doc)\n",
    "    d=[]\n",
    "    for token in nlp_doc:\n",
    "        if(not token.text.lower()  in STOP_WORDS and  token.text.isalpha()):\n",
    "            d.append(token.lemma_.lower() )\n",
    "    return ' '.join(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(\"skin peeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_symp_pr=[preprocess_sym(sym) for sym in all_symp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#associe chaque symp pretraite au non de sa colonne originale\n",
    "col_dict = dict(zip(all_symp_pr, all_symp_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III- Syntactic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_set(str1, str2):\n",
    "    list1=str1.split(' ')\n",
    "    list2=str2.split(' ')\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarite syn avec ts le corpus\n",
    "def syntactic_similarity(symp_t, corpus):\n",
    "    most_sim = []\n",
    "    poss_sym = []\n",
    "    for symp in corpus:\n",
    "        d = jaccard_set(symp_t, symp)\n",
    "        most_sim.append(d)\n",
    "    order = np.argsort(most_sim)[::-1].tolist()\n",
    "    for i in order:\n",
    "        if DoesExist(symp_t):\n",
    "            return 1, [corpus[i]]\n",
    "        if corpus[i] not in poss_sym and most_sim[i] != 0:\n",
    "            poss_sym.append(corpus[i])\n",
    "    if len(poss_sym):\n",
    "        return 1, poss_sym\n",
    "    else:\n",
    "        return 0, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#Returns all the subsets of this set. This is a generator.\n",
    "def powerset(seq):\n",
    "    if len(seq) <= 1:\n",
    "        yield seq\n",
    "        yield []\n",
    "    else:\n",
    "        for item in powerset(seq[1:]):\n",
    "            yield [seq[0]]+item\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort list based on length\n",
    "def sort(a):\n",
    "    for i in range(len(a)):\n",
    "        for j in range(i+1,len(a)):\n",
    "            if len(a[j])>len(a[i]):\n",
    "                a[i],a[j]=a[j],a[i]\n",
    "    a.pop()\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all permutations of a list\n",
    "def permutations(s):\n",
    "    permutations = list(itertools.permutations(s))\n",
    "    return([' '.join(permutation) for permutation in permutations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoesExist(txt):\n",
    "    txt=txt.split(' ')\n",
    "    combinations = [x for x in powerset(txt)]\n",
    "    sort(combinations)\n",
    "    for comb in combinations :\n",
    "        #print(permutations(comb))\n",
    "        for sym in permutations(comb):\n",
    "            if sym in all_symp_pr:\n",
    "                #print(sym)\n",
    "                return sym\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DoesExist('worried')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess('really worried')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntactic_similarity(preprocess('nervous') ,all_symp_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pattern(inp,dis_list):\n",
    "    import re\n",
    "    pred_list=[]\n",
    "    ptr=0\n",
    "    patt = \"^\" + inp + \"$\"\n",
    "    regexp = re.compile(inp)\n",
    "    for item in dis_list:\n",
    "        if regexp.search(item):\n",
    "            pred_list.append(item)\n",
    "    if(len(pred_list)>0):\n",
    "        return 1,pred_list\n",
    "    else:\n",
    "        return ptr,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_pattern('nail',all_symp_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV- Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "def WSD(word, context):\n",
    "    sens=lesk(context, word)\n",
    "    return sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semanticD(doc1,doc2):\n",
    "    doc1_p=preprocess(doc1).split(' ')\n",
    "    doc2_p=preprocess_sym(doc2).split(' ')\n",
    "    score=0\n",
    "    for tock1 in doc1_p:\n",
    "        for tock2 in doc2_p:\n",
    "            syn1 = WSD(tock1,doc1)\n",
    "            syn2 = WSD(tock2,doc2)\n",
    "            #syn1=wn.synset(t)\n",
    "            if syn1 is not None and syn2 is not None :\n",
    "                x=syn1.wup_similarity(syn2)\n",
    "                if x is not None and x>0.1:\n",
    "                    score+=x\n",
    "    return score/(len(doc1_p)*len(doc2_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semanticD('anxiety','nervous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syna=wn.synsets('anxiety')\n",
    "syna[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synn=wn.synsets('nervous')\n",
    "synn[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synn[0].wup_similarity(syna[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anxiety_synsets = wn.synsets(\"brittle\") \n",
    "nervous_synsets = wn.synsets(\"nervous\") \n",
    "path=[]\n",
    "wup=[]\n",
    "lch=[]\n",
    "\n",
    "\n",
    "for s1 in anxiety_synsets:\n",
    "    for s2 in nervous_synsets:\n",
    "        path.append(s1.path_similarity(s2))\n",
    "        wup.append(s1.wup_similarity(s2))\n",
    "        #lch.append(s1.lch_similarity(s2))\n",
    "        \n",
    "\n",
    "pd.DataFrame([path,wup],[\"path\",\"wup\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarite sem avec ts le corpus\n",
    "def semantic_similarity(symp_t,corpus):\n",
    "    max_sim=0\n",
    "    most_sim=None\n",
    "    for symp in corpus:\n",
    "        d=semanticD(symp_t,symp)\n",
    "        if d>max_sim:\n",
    "            most_sim=symp\n",
    "            max_sim=d\n",
    "    return max_sim,most_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_similarity('nervous',all_symp_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_symp_pr.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_symp_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from nltk.corpus import wordnet\n",
    "def suggest_syn(sym):\n",
    "    symp=[]\n",
    "    synonyms = wordnet.synsets(sym)\n",
    "    lemmas=[word.lemma_names() for word in synonyms]\n",
    "    lemmas = list(set(chain(*lemmas)))\n",
    "    for e in lemmas:\n",
    "        res,sym1=semantic_similarity(e,all_symp_pr)\n",
    "        if res!=0:\n",
    "            symp.append(sym1)\n",
    "    return list(set(symp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_syn('worried')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recoit client_symptoms et renvoit un dataframe avec 1 pour les symptoms associees\n",
    "def OHV(cl_sym,all_sym):\n",
    "    l=np.zeros([1,len(all_sym)])\n",
    "    for sym in cl_sym:\n",
    "        l[0,all_sym.index(sym)]=1\n",
    "    return pd.DataFrame(l, columns =all_symp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains(small, big):\n",
    "    a=True\n",
    "    for i in small:\n",
    "        if i not in big:\n",
    "            a=False\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_diseases(l):\n",
    "    poss_dis=[]\n",
    "    for dis in set(disease):\n",
    "        if contains(l,symVONdisease(df_tr,dis)):\n",
    "            poss_dis.append(dis)\n",
    "    return poss_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recoit une maladie renvoit tous les sympts\n",
    "def symVONdisease(df,disease):\n",
    "    ddf=df[df.prognosis==disease]\n",
    "    m2 = (ddf == 1).any()\n",
    "    return m2.index[m2].tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symVONdisease(df_tr,'Jaundice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V- Prediction Model (KNN & DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df_tr.iloc[:,:-1]\n",
    "X_test=df_tt.iloc[:,:-1]\n",
    "y_train = df_tr.iloc[:,-1]\n",
    "y_test = df_tt.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf=KNeighborsClassifier()\n",
    "knn_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,knn_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,dt_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  VI- SEVERITY / DESCRIPTION / PRECAUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severityDictionary=dict()\n",
    "description_list = dict()\n",
    "precautionDictionary=dict()\n",
    "\n",
    "def getDescription():\n",
    "    global description_list\n",
    "    with open('Medical_dataset/symptom_Description.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            _description={row[0]:row[1]}\n",
    "            description_list.update(_description)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getSeverityDict():\n",
    "    global severityDictionary\n",
    "    with open('Medical_dataset/symptom_severity.csv') as csv_file:\n",
    "\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        try:\n",
    "            for row in csv_reader:\n",
    "                _diction={row[0]:int(row[1])}\n",
    "                severityDictionary.update(_diction)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "def getprecautionDict():\n",
    "    global precautionDictionary\n",
    "    with open('Medical_dataset/symptom_precaution.csv') as csv_file:\n",
    "\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            _prec={row[0]:[row[1],row[2],row[3],row[4]]}\n",
    "            precautionDictionary.update(_prec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSeverityDict()\n",
    "getprecautionDict()\n",
    "getDescription()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severityDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_condition(exp,days):\n",
    "    sum=0\n",
    "    for item in exp:\n",
    "        if item in severityDictionary.keys():\n",
    "            sum=sum+severityDictionary[item]\n",
    "    if((sum*days)/(len(exp))>13):\n",
    "        return 1\n",
    "        print(\"You should take the consultation from doctor. \")\n",
    "    else:\n",
    "        return 0\n",
    "        print(\"It might not be that bad but you should take precautions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfo():\n",
    "    # name=input(\"Name:\")\n",
    "    print(\"Your Name \\n\\t\\t\\t\\t\\t\\t\",end=\"=>\")\n",
    "    name=input(\"\")\n",
    "    print(\"hello \",name)\n",
    "    return str(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def related_sym(psym1):\n",
    "    if len(psym1)==1:\n",
    "        return psym1[0]\n",
    "    print(\"searches related to input: \")\n",
    "    for num,it in enumerate(psym1):\n",
    "        print(num,\")\",clean_symp(it))\n",
    "    if num!=0:\n",
    "        print(f\"Select the one you meant (0 - {num}):  \", end=\"\")\n",
    "        conf_inp = int(input(\"\"))\n",
    "    else:\n",
    "        conf_inp=0\n",
    "\n",
    "    disease_input=psym1[conf_inp]\n",
    "    return disease_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_sp(name,all_symp_col):\n",
    "    #main Idea: At least two initial sympts to start with\n",
    "    \n",
    "    #get the 1st syp ->> process it ->> check_pattern ->>> get the appropriate one (if check_pattern==1 == similar syntaxic symp found)\n",
    "    print(\"Enter the main symptom you are experiencing Mr/Ms \"+name+\"  \\n\\t\\t\\t\\t\\t\\t\",end=\"=>\")\n",
    "    sym1 = input(\"\")\n",
    "    sym1=preprocess_sym(sym1)\n",
    "    sim1,psym1=syntactic_similarity(sym1,all_symp_pr)\n",
    "    if sim1==1:\n",
    "        psym1=related_sym(psym1)\n",
    "    \n",
    "    #get the 2nd syp ->> process it ->> check_pattern ->>> get the appropriate one (if check_pattern==1 == similar syntaxic symp found)\n",
    "\n",
    "    print(\"Enter a second symptom you are experiencing Mr/Ms \"+name+\"  \\n\\t\\t\\t\\t\\t\\t\",end=\"=>\")\n",
    "    sym2=input(\"\")\n",
    "    sym2=preprocess_sym(sym2)\n",
    "    sim2,psym2=syntactic_similarity(sym2,all_symp_pr)\n",
    "    if sim2==1:\n",
    "        psym2=related_sym(psym2)\n",
    "        \n",
    "    #if check_pattern==0 no similar syntaxic symp1 or symp2 ->> try semantic similarity\n",
    "    \n",
    "    if sim1==0 or sim2==0:\n",
    "        sim1,psym1=semantic_similarity(sym1,all_symp_pr)\n",
    "        sim2,psym2=semantic_similarity(sym2,all_symp_pr)\n",
    "        \n",
    "        #if semantic sim syp1 ==0 (no symp found) ->> suggest possible data symptoms based on all data and input sym synonymes\n",
    "        if sim1==0:\n",
    "            sugg=suggest_syn(sym1)\n",
    "            print('Are you experiencing any ')\n",
    "            for res in sugg:\n",
    "                print(res)\n",
    "                inp=input('')\n",
    "                if inp==\"yes\":\n",
    "                    psym1=res\n",
    "                    sim1=1\n",
    "                    break\n",
    "                \n",
    "        #if semantic sim syp2 ==0 (no symp found) ->> suggest possible data symptoms based on all data and input sym synonymes\n",
    "        if sim2==0:\n",
    "            sugg=suggest_syn(sym2)\n",
    "            for res in sugg:\n",
    "                inp=input('Do you feel '+ res+\" ?(yes or no) \")\n",
    "                if inp==\"yes\":\n",
    "                    psym2=res\n",
    "                    sim2=1\n",
    "                    break\n",
    "        #if no syntaxic semantic and suggested sym found return None and ask for clarification\n",
    "\n",
    "        if sim1==0 and sim2==0:\n",
    "            return None,None\n",
    "        else:\n",
    "            # if at least one sym found ->> duplicate it and proceed\n",
    "            if sim1==0:\n",
    "                psym1=psym2\n",
    "            if sim2==0:\n",
    "                psym2=psym1\n",
    "    #create patient symp list\n",
    "    all_sym=[col_dict[psym1],col_dict[psym2]]\n",
    "    #predict possible diseases\n",
    "    diseases=possible_diseases(all_sym)\n",
    "    stop=False\n",
    "    print(\"Are you experiencing any \")\n",
    "    for dis in diseases:\n",
    "        print(diseases)\n",
    "        if stop==False:\n",
    "            for sym in symVONdisease(df_tr,dis):\n",
    "                if sym not in all_sym:\n",
    "                    print(clean_symp(sym)+' ?')\n",
    "                    while True:\n",
    "                        inp=input(\"\")\n",
    "                        if(inp==\"yes\" or inp==\"no\"):\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"provide proper answers i.e. (yes/no) : \",end=\"\")\n",
    "                    if inp==\"yes\":\n",
    "                        all_sym.append(sym)\n",
    "                        diseases=possible_diseases(all_sym)\n",
    "                        if len(diseases)==1:\n",
    "                            stop=True \n",
    "    return knn_clf.predict(OHV(all_sym,all_symp_col)),all_sym\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_sp():\n",
    "    a=True\n",
    "    while a:\n",
    "        name=getInfo()\n",
    "        result,sym=main_sp(name,all_symp_col)\n",
    "        if result == None :\n",
    "            ans3=input(\"can you specify more what you feel or tap q to stop the conversation\")\n",
    "            if ans3==\"q\":\n",
    "                a=False\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            print(\"you may have \"+result[0])\n",
    "            print(description_list[result[0]])\n",
    "            an=input(\"how many day do you feel those symptoms ?\")\n",
    "            if calc_condition(sym,int(an))==1:\n",
    "                print(\"you should take the consultation from doctor\")\n",
    "            else : \n",
    "                print('Take following precautions : ')\n",
    "                for e in precautionDictionary[result[0]]:\n",
    "                    print(e)\n",
    "            print(\"do you need another medical consultation (yes or no)? \")\n",
    "            ans=input()\n",
    "            if ans!=\"yes\":\n",
    "                a=False\n",
    "                print(\"!!!!! thanks for using ower application !!!!!! \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "knn_clf=joblib.load('model/knn.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symVONdisease(df_tr,\"Jaundice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.predict(OHV(['fatigue', 'weight_loss', 'itching','high_fever'],all_symp_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=df_tr[df_tr.iloc[:,-1]==\"Fungal infection\"].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl=df_tr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp=d!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl[pp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[pp].drop('prognosis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_sp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
